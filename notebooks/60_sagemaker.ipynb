{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98180e1e",
   "metadata": {},
   "source": [
    "# Sagemaker Training and Inference\n",
    "\n",
    "In this notebook, I give a short demonstration of how to train and deploy the residual CNN model on Amazon SageMaker.\n",
    "\n",
    "**This notebook:**\n",
    "- Uploads training data to an S3 bucket\n",
    "- Trains a model on SageMaker\n",
    "- Deploys the trained model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21112930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "from botocore.client import BaseClient\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "\n",
    "n_actions = 1\n",
    "prefix = f\"pdl/cnn1d-residual-n{n_actions}/v1\"\n",
    "framework_version = \"2.6\"\n",
    "py_version = \"py312\"\n",
    "\n",
    "session = sagemaker.Session()  # Use LocalSession() for local deployment testing\n",
    "role = os.getenv(\"SAGEMAKER_ROLE_ARN\")\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "data_s3 = f\"s3://{bucket}/{prefix}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0772b",
   "metadata": {},
   "source": [
    "### Upload\n",
    "\n",
    "First, I upload the training data to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(f\"data/n{n_actions}/processed\")\n",
    "\n",
    "for path in data_path.glob(\"*.json\"):\n",
    "    file_path = f\"{prefix}/{path.name}\"\n",
    "    s3.upload_file(str(path), bucket, file_path)\n",
    "    print(f\"Uploaded {path} to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6467c4fd",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next, I train the model on SageMaker, and the torchscript-compiled model is saved, using the training script `train_cnn1d_residual.py`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76371db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2026-01-11-14-38-37-129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 14:38:39 Starting - Starting the training job...\n",
      "2026-01-11 14:38:54 Starting - Preparing the instances for training...\n",
      "2026-01-11 14:39:17 Downloading - Downloading input data...\n",
      "2026-01-11 14:39:57 Downloading - Downloading the training image......\n",
      "2026-01-11 14:41:08 Training - Training image download completed. Training in progress..bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2026-01-11 14:41:11,764 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2026-01-11 14:41:11,765 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2026-01-11 14:41:11,766 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2026-01-11 14:41:11,776 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2026-01-11 14:41:11,809 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2026-01-11 14:41:13,746 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\n",
      "Collecting mlflow>=2.8.0 (from -r requirements.txt (line 6))\n",
      "Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting numpydantic>=1.7.0 (from -r requirements.txt (line 8))\n",
      "Downloading numpydantic-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting optuna>=3.4.0 (from -r requirements.txt (line 12))\n",
      "Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting mlflow-skinny==3.8.1 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.8.1 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Flask<4 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading alembic-1.18.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (45.0.5)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (3.4.3)\n",
      "Collecting gunicorn<24 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting huey<3,>=2.5.0 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (3.10.3)\n",
      "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (2.3.1)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (20.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (1.7.0)\n",
      "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/site-packages (from mlflow>=2.8.0->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.1.1)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading databricks_sdk-0.77.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.128.0)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (6.11.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (24.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.11.7)\n",
      "Collecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.32.4)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (4.14.1)\n",
      "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/site-packages (from mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.35.0)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in /usr/local/lib/python3.12/site-packages (from cryptography<47,>=43.0.0->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.17.1)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/site-packages (from docker<8,>=4.0.0->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.6.2)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/site-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.0.4)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/site-packages (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/site-packages (from Flask<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (4.7.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/site-packages (from graphene<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/site-packages (from graphene<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/site-packages (from matplotlib<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.2.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6))\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas<3->mlflow>=2.8.0->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas<3->mlflow>=2.8.0->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (2025.7.14)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn<2->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.6.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8.0->-r requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (4.9.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow>=2.8.0->-r requirements.txt (line 6)) (0.16.0)\n",
      "Collecting colorlog (from optuna>=3.4.0->-r requirements.txt (line 12))\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/site-packages (from optuna>=3.4.0->-r requirements.txt (line 12)) (4.67.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi>=1.14->cryptography<47,>=43.0.0->mlflow>=2.8.0->-r requirements.txt (line 6)) (2.22)\n",
      "Downloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.1/9.1 MB 115.5 MB/s  0:00:00\n",
      "Downloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 54.3 MB/s  0:00:00\n",
      "Downloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 20.2 MB/s  0:00:00\n",
      "Downloading alembic-1.18.0-py3-none-any.whl (260 kB)\n",
      "Downloading cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
      "Downloading databricks_sdk-0.77.0-py3-none-any.whl (779 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 779.2/779.2 kB 44.4 MB/s  0:00:00\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
      "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Downloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 113.9 MB/s  0:00:00\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "Downloading numpydantic-1.7.0-py3-none-any.whl (86 kB)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: huey, sqlparse, sqlalchemy, smmap, python-dotenv, pyasn1-modules, opentelemetry-proto, Mako, itsdangerous, gunicorn, colorlog, cachetools, blinker, opentelemetry-api, google-auth, gitdb, Flask, alembic, optuna, opentelemetry-semantic-conventions, numpydantic, gitpython, Flask-CORS, databricks-sdk, opentelemetry-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 alembic-1.18.0 blinker-1.9.0 cachetools-6.2.4 colorlog-6.10.1 databricks-sdk-0.77.0 gitdb-4.0.12 gitpython-3.1.46 google-auth-2.47.0 gunicorn-23.0.0 huey-2.6.0 itsdangerous-2.2.0 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1 numpydantic-1.7.0 opentelemetry-api-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 optuna-4.6.0 pyasn1-modules-0.4.2 python-dotenv-1.2.1 smmap-5.0.2 sqlalchemy-2.0.45 sqlparse-0.5.5\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "2026-01-11 14:41:25,220 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2026-01-11 14:41:25,220 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2026-01-11 14:41:25,222 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2026-01-11 14:41:25,222 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2026-01-11 14:41:25,232 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2026-01-11 14:41:25,233 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2026-01-11 14:41:25,243 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2026-01-11 14:41:25,244 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2026-01-11 14:41:25,253 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"processed\": \"/opt/ml/input/data/processed\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"dropout\": 0.3,\n",
      "        \"epochs\": 200,\n",
      "        \"filters\": \"64 64 196\",\n",
      "        \"hidden-dims\": \"64 96\",\n",
      "        \"kernel-sizes\": \"3 5 7\",\n",
      "        \"lr\": 0.01,\n",
      "        \"patience\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"processed\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2026-01-11-14-38-37-129\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-857754129070/pytorch-training-2026-01-11-14-38-37-129/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"preference_dynamics/sagemaker/train_cnn1d_residual\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"preference_dynamics/sagemaker/train_cnn1d_residual.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch-size\":32,\"dropout\":0.3,\"epochs\":200,\"filters\":\"64 64 196\",\"hidden-dims\":\"64 96\",\"kernel-sizes\":\"3 5 7\",\"lr\":0.01,\"patience\":10}\n",
      "SM_USER_ENTRY_POINT=preference_dynamics/sagemaker/train_cnn1d_residual.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"processed\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"processed\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m5.large\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=preference_dynamics/sagemaker/train_cnn1d_residual\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=2\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-eu-central-1-857754129070/pytorch-training-2026-01-11-14-38-37-129/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"processed\":\"/opt/ml/input/data/processed\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":32,\"dropout\":0.3,\"epochs\":200,\"filters\":\"64 64 196\",\"hidden-dims\":\"64 96\",\"kernel-sizes\":\"3 5 7\",\"lr\":0.01,\"patience\":10},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"processed\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2026-01-11-14-38-37-129\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-857754129070/pytorch-training-2026-01-11-14-38-37-129/source/sourcedir.tar.gz\",\"module_name\":\"preference_dynamics/sagemaker/train_cnn1d_residual\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"preference_dynamics/sagemaker/train_cnn1d_residual.py\"}\n",
      "SM_USER_ARGS=[\"--batch-size\",\"32\",\"--dropout\",\"0.3\",\"--epochs\",\"200\",\"--filters\",\"64 64 196\",\"--hidden-dims\",\"64 96\",\"--kernel-sizes\",\"3 5 7\",\"--lr\",\"0.01\",\"--patience\",\"10\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_PROCESSED=/opt/ml/input/data/processed\n",
      "SM_HP_BATCH-SIZE=32\n",
      "SM_HP_DROPOUT=0.3\n",
      "SM_HP_EPOCHS=200\n",
      "SM_HP_FILTERS=64 64 196\n",
      "SM_HP_HIDDEN-DIMS=64 96\n",
      "SM_HP_KERNEL-SIZES=3 5 7\n",
      "SM_HP_LR=0.01\n",
      "SM_HP_PATIENCE=10\n",
      "PYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python312.zip:/usr/local/lib/python3.12:/usr/local/lib/python3.12/lib-dynload:/usr/local/lib/python3.12/site-packages\n",
      "Invoking script with the following command:\n",
      "/usr/local/bin/python preference_dynamics/sagemaker/train_cnn1d_residual.py --batch-size 32 --dropout 0.3 --epochs 200 --filters 64 64 196 --hidden-dims 64 96 --kernel-sizes 3 5 7 --lr 0.01 --patience 10\n",
      "2026-01-11 14:41:25,255 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2026-01-11 14:41:25,255 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Training:   0%|          | 0/200 [00:00<?, ?it/s, epoch=0, train_loss=23.3, val_loss=25.2, epoch_time=0.278]\n",
      "Training:   0%|          | 1/200 [00:00<00:57,  3.49it/s, epoch=0, train_loss=23.3, val_loss=25.2, epoch_time=0.278]\n",
      "Training:   0%|          | 1/200 [00:00<00:57,  3.49it/s, epoch=1, train_loss=19.2, val_loss=23.6, epoch_time=0.252]\n",
      "Training:   1%|          | 2/200 [00:00<00:51,  3.87it/s, epoch=1, train_loss=19.2, val_loss=23.6, epoch_time=0.252]\n",
      "Training:   1%|          | 2/200 [00:00<00:51,  3.87it/s, epoch=2, train_loss=15.8, val_loss=23.7, epoch_time=0.226]\n",
      "Training:   2%|▏         | 3/200 [00:00<00:44,  4.41it/s, epoch=2, train_loss=15.8, val_loss=23.7, epoch_time=0.226]\n",
      "Training:   2%|▏         | 3/200 [00:01<00:44,  4.41it/s, epoch=3, train_loss=12.7, val_loss=20.5, epoch_time=0.23]\n",
      "Training:   2%|▏         | 4/200 [00:01<00:46,  4.22it/s, epoch=3, train_loss=12.7, val_loss=20.5, epoch_time=0.23]\n",
      "Training:   2%|▏         | 4/200 [00:01<00:46,  4.22it/s, epoch=4, train_loss=10.8, val_loss=18.2, epoch_time=0.232]\n",
      "Training:   2%|▎         | 5/200 [00:01<00:46,  4.20it/s, epoch=4, train_loss=10.8, val_loss=18.2, epoch_time=0.232]\n",
      "Training:   2%|▎         | 5/200 [00:01<00:46,  4.20it/s, epoch=5, train_loss=8.75, val_loss=17.3, epoch_time=0.245]\n",
      "Training:   3%|▎         | 6/200 [00:01<00:48,  3.97it/s, epoch=5, train_loss=8.75, val_loss=17.3, epoch_time=0.245]\n",
      "Training:   3%|▎         | 6/200 [00:01<00:48,  3.97it/s, epoch=6, train_loss=7.4, val_loss=14.9, epoch_time=0.23]\n",
      "Training:   4%|▎         | 7/200 [00:01<00:45,  4.24it/s, epoch=6, train_loss=7.4, val_loss=14.9, epoch_time=0.23]\n",
      "Training:   4%|▎         | 7/200 [00:01<00:45,  4.24it/s, epoch=7, train_loss=6.33, val_loss=13.4, epoch_time=0.231]\n",
      "Training:   4%|▍         | 8/200 [00:01<00:45,  4.22it/s, epoch=7, train_loss=6.33, val_loss=13.4, epoch_time=0.231]\n",
      "Training:   4%|▍         | 8/200 [00:02<00:45,  4.22it/s, epoch=8, train_loss=5.73, val_loss=12.2, epoch_time=0.233]\n",
      "Training:   4%|▍         | 9/200 [00:02<00:45,  4.19it/s, epoch=8, train_loss=5.73, val_loss=12.2, epoch_time=0.233]\n",
      "Training:   4%|▍         | 9/200 [00:02<00:45,  4.19it/s, epoch=9, train_loss=5.07, val_loss=11.2, epoch_time=0.231]\n",
      "Training:   5%|▌         | 10/200 [00:02<00:45,  4.21it/s, epoch=9, train_loss=5.07, val_loss=11.2, epoch_time=0.231]\n",
      "Training:   5%|▌         | 10/200 [00:02<00:45,  4.21it/s, epoch=10, train_loss=4.48, val_loss=10.7, epoch_time=0.228]#015Training:   6%|▌         | 11/200 [00:02<00:44,  4.27it/s, epoch=10, train_loss=4.48, val_loss=10.7, epoch_time=0.228]\n",
      "Training:   6%|▌         | 11/200 [00:02<00:44,  4.27it/s, epoch=11, train_loss=4.5, val_loss=9.83, epoch_time=0.227]\n",
      "Training:   6%|▌         | 12/200 [00:02<00:43,  4.28it/s, epoch=11, train_loss=4.5, val_loss=9.83, epoch_time=0.227]\n",
      "Training:   6%|▌         | 12/200 [00:03<00:43,  4.28it/s, epoch=12, train_loss=4.12, val_loss=9.57, epoch_time=0.229]\n",
      "Training:   6%|▋         | 13/200 [00:03<00:44,  4.24it/s, epoch=12, train_loss=4.12, val_loss=9.57, epoch_time=0.229]\n",
      "Training:   6%|▋         | 13/200 [00:03<00:44,  4.24it/s, epoch=13, train_loss=3.9, val_loss=8.77, epoch_time=0.241]\n",
      "Training:   7%|▋         | 14/200 [00:03<00:46,  4.03it/s, epoch=13, train_loss=3.9, val_loss=8.77, epoch_time=0.241]\n",
      "Training:   7%|▋         | 14/200 [00:03<00:46,  4.03it/s, epoch=14, train_loss=3.66, val_loss=8.17, epoch_time=0.245]\n",
      "Training:   8%|▊         | 15/200 [00:03<00:46,  3.98it/s, epoch=14, train_loss=3.66, val_loss=8.17, epoch_time=0.245]\n",
      "Training:   8%|▊         | 15/200 [00:03<00:46,  3.98it/s, epoch=15, train_loss=3.3, val_loss=8.16, epoch_time=0.232]\n",
      "Training:   8%|▊         | 16/200 [00:03<00:43,  4.19it/s, epoch=15, train_loss=3.3, val_loss=8.16, epoch_time=0.232]\n",
      "Training:   8%|▊         | 16/200 [00:04<00:43,  4.19it/s, epoch=16, train_loss=3.1, val_loss=7.41, epoch_time=0.236]\n",
      "Training:   8%|▊         | 17/200 [00:04<00:44,  4.13it/s, epoch=16, train_loss=3.1, val_loss=7.41, epoch_time=0.236]\n",
      "Training:   8%|▊         | 17/200 [00:04<00:44,  4.13it/s, epoch=17, train_loss=2.55, val_loss=6.94, epoch_time=0.24]\n",
      "Training:   9%|▉         | 18/200 [00:04<00:44,  4.05it/s, epoch=17, train_loss=2.55, val_loss=6.94, epoch_time=0.24]\n",
      "Training:   9%|▉         | 18/200 [00:04<00:44,  4.05it/s, epoch=18, train_loss=2.42, val_loss=6.44, epoch_time=0.25]\n",
      "Training:  10%|▉         | 19/200 [00:04<00:46,  3.89it/s, epoch=18, train_loss=2.42, val_loss=6.44, epoch_time=0.25]\n",
      "Training:  10%|▉         | 19/200 [00:04<00:46,  3.89it/s, epoch=19, train_loss=2.16, val_loss=6.03, epoch_time=0.229]\n",
      "Training:  10%|█         | 20/200 [00:04<00:42,  4.24it/s, epoch=19, train_loss=2.16, val_loss=6.03, epoch_time=0.229]\n",
      "Training:  10%|█         | 20/200 [00:05<00:42,  4.24it/s, epoch=20, train_loss=1.9, val_loss=5.88, epoch_time=0.23]\n",
      "Training:  10%|█         | 21/200 [00:05<00:42,  4.23it/s, epoch=20, train_loss=1.9, val_loss=5.88, epoch_time=0.23]\n",
      "Training:  10%|█         | 21/200 [00:05<00:42,  4.23it/s, epoch=21, train_loss=1.86, val_loss=5.21, epoch_time=0.234]\n",
      "Training:  11%|█         | 22/200 [00:05<00:42,  4.15it/s, epoch=21, train_loss=1.86, val_loss=5.21, epoch_time=0.234]\n",
      "Training:  11%|█         | 22/200 [00:05<00:42,  4.15it/s, epoch=22, train_loss=1.78, val_loss=4.86, epoch_time=0.237]\n",
      "Training:  12%|█▏        | 23/200 [00:05<00:43,  4.10it/s, epoch=22, train_loss=1.78, val_loss=4.86, epoch_time=0.237]\n",
      "Training:  12%|█▏        | 23/200 [00:05<00:43,  4.10it/s, epoch=23, train_loss=1.55, val_loss=4.52, epoch_time=0.227]\n",
      "Training:  12%|█▏        | 24/200 [00:05<00:40,  4.29it/s, epoch=23, train_loss=1.55, val_loss=4.52, epoch_time=0.227]\n",
      "Training:  12%|█▏        | 24/200 [00:06<00:40,  4.29it/s, epoch=24, train_loss=1.51, val_loss=4.26, epoch_time=0.23]\n",
      "Training:  12%|█▎        | 25/200 [00:06<00:41,  4.23it/s, epoch=24, train_loss=1.51, val_loss=4.26, epoch_time=0.23]\n",
      "Training:  12%|█▎        | 25/200 [00:06<00:41,  4.23it/s, epoch=25, train_loss=1.47, val_loss=4.03, epoch_time=0.229]\n",
      "Training:  13%|█▎        | 26/200 [00:06<00:40,  4.25it/s, epoch=25, train_loss=1.47, val_loss=4.03, epoch_time=0.229]\n",
      "Training:  13%|█▎        | 26/200 [00:06<00:40,  4.25it/s, epoch=26, train_loss=1.36, val_loss=3.87, epoch_time=0.242]\n",
      "Training:  14%|█▎        | 27/200 [00:06<00:42,  4.03it/s, epoch=26, train_loss=1.36, val_loss=3.87, epoch_time=0.242]\n",
      "Training:  14%|█▎        | 27/200 [00:06<00:42,  4.03it/s, epoch=27, train_loss=1.28, val_loss=3.71, epoch_time=0.228]\n",
      "Training:  14%|█▍        | 28/200 [00:06<00:40,  4.26it/s, epoch=27, train_loss=1.28, val_loss=3.71, epoch_time=0.228]\n",
      "Training:  14%|█▍        | 28/200 [00:07<00:40,  4.26it/s, epoch=28, train_loss=1.3, val_loss=3.41, epoch_time=0.229]\n",
      "Training:  14%|█▍        | 29/200 [00:07<00:40,  4.26it/s, epoch=28, train_loss=1.3, val_loss=3.41, epoch_time=0.229]\n",
      "Training:  14%|█▍        | 29/200 [00:07<00:40,  4.26it/s, epoch=29, train_loss=1.17, val_loss=3.2, epoch_time=0.223]\n",
      "Training:  15%|█▌        | 30/200 [00:07<00:39,  4.35it/s, epoch=29, train_loss=1.17, val_loss=3.2, epoch_time=0.223]\n",
      "Training:  15%|█▌        | 30/200 [00:07<00:39,  4.35it/s, epoch=30, train_loss=1.11, val_loss=2.89, epoch_time=0.228]\n",
      "Training:  16%|█▌        | 31/200 [00:07<00:39,  4.26it/s, epoch=30, train_loss=1.11, val_loss=2.89, epoch_time=0.228]\n",
      "Training:  16%|█▌        | 31/200 [00:07<00:39,  4.26it/s, epoch=31, train_loss=1.11, val_loss=2.73, epoch_time=0.235]\n",
      "Training:  16%|█▌        | 32/200 [00:07<00:40,  4.14it/s, epoch=31, train_loss=1.11, val_loss=2.73, epoch_time=0.235]\n",
      "Training:  16%|█▌        | 32/200 [00:07<00:40,  4.14it/s, epoch=32, train_loss=1.01, val_loss=2.63, epoch_time=0.223]\n",
      "Training:  16%|█▋        | 33/200 [00:07<00:39,  4.26it/s, epoch=32, train_loss=1.01, val_loss=2.63, epoch_time=0.223]\n",
      "Training:  16%|█▋        | 33/200 [00:08<00:39,  4.26it/s, epoch=33, train_loss=0.959, val_loss=2.33, epoch_time=0.23]\n",
      "Training:  17%|█▋        | 34/200 [00:08<00:39,  4.23it/s, epoch=33, train_loss=0.959, val_loss=2.33, epoch_time=0.23]\n",
      "Training:  17%|█▋        | 34/200 [00:08<00:39,  4.23it/s, epoch=34, train_loss=0.899, val_loss=2.22, epoch_time=0.229]\n",
      "Training:  18%|█▊        | 35/200 [00:08<00:38,  4.25it/s, epoch=34, train_loss=0.899, val_loss=2.22, epoch_time=0.229]\n",
      "Training:  18%|█▊        | 35/200 [00:08<00:38,  4.25it/s, epoch=35, train_loss=0.853, val_loss=2.05, epoch_time=0.234]\n",
      "Training:  18%|█▊        | 36/200 [00:08<00:39,  4.17it/s, epoch=35, train_loss=0.853, val_loss=2.05, epoch_time=0.234]\n",
      "Training:  18%|█▊        | 36/200 [00:08<00:39,  4.17it/s, epoch=36, train_loss=0.889, val_loss=1.95, epoch_time=0.23]\n",
      "Training:  18%|█▊        | 37/200 [00:08<00:38,  4.23it/s, epoch=36, train_loss=0.889, val_loss=1.95, epoch_time=0.23]\n",
      "Training:  18%|█▊        | 37/200 [00:09<00:38,  4.23it/s, epoch=37, train_loss=0.853, val_loss=1.97, epoch_time=0.232]\n",
      "Training:  19%|█▉        | 38/200 [00:09<00:37,  4.30it/s, epoch=37, train_loss=0.853, val_loss=1.97, epoch_time=0.232]\n",
      "Training:  19%|█▉        | 38/200 [00:09<00:37,  4.30it/s, epoch=38, train_loss=0.768, val_loss=1.88, epoch_time=0.243]\n",
      "Training:  20%|█▉        | 39/200 [00:09<00:40,  4.01it/s, epoch=38, train_loss=0.768, val_loss=1.88, epoch_time=0.243]\n",
      "Training:  20%|█▉        | 39/200 [00:09<00:40,  4.01it/s, epoch=39, train_loss=0.745, val_loss=1.73, epoch_time=0.253]\n",
      "Training:  20%|██        | 40/200 [00:09<00:41,  3.85it/s, epoch=39, train_loss=0.745, val_loss=1.73, epoch_time=0.253]\n",
      "Training:  20%|██        | 40/200 [00:09<00:41,  3.85it/s, epoch=40, train_loss=0.693, val_loss=1.95, epoch_time=0.22]\n",
      "Training:  20%|██        | 41/200 [00:09<00:35,  4.53it/s, epoch=40, train_loss=0.693, val_loss=1.95, epoch_time=0.22]\n",
      "Training:  20%|██        | 41/200 [00:10<00:35,  4.53it/s, epoch=41, train_loss=0.679, val_loss=1.41, epoch_time=0.233]\n",
      "Training:  21%|██        | 42/200 [00:10<00:37,  4.17it/s, epoch=41, train_loss=0.679, val_loss=1.41, epoch_time=0.233]\n",
      "Training:  21%|██        | 42/200 [00:10<00:37,  4.17it/s, epoch=42, train_loss=0.625, val_loss=1.43, epoch_time=0.236]\n",
      "Training:  22%|██▏       | 43/200 [00:10<00:37,  4.23it/s, epoch=42, train_loss=0.625, val_loss=1.43, epoch_time=0.236]\n",
      "Training:  22%|██▏       | 43/200 [00:10<00:37,  4.23it/s, epoch=43, train_loss=0.692, val_loss=1.23, epoch_time=0.244]\n",
      "Training:  22%|██▏       | 44/200 [00:10<00:39,  4.00it/s, epoch=43, train_loss=0.692, val_loss=1.23, epoch_time=0.244]\n",
      "Training:  22%|██▏       | 44/200 [00:10<00:39,  4.00it/s, epoch=44, train_loss=0.608, val_loss=1.23, epoch_time=0.234]\n",
      "Training:  22%|██▎       | 45/200 [00:10<00:36,  4.26it/s, epoch=44, train_loss=0.608, val_loss=1.23, epoch_time=0.234]\n",
      "Training:  22%|██▎       | 45/200 [00:11<00:36,  4.26it/s, epoch=45, train_loss=0.577, val_loss=1.11, epoch_time=0.222]\n",
      "Training:  23%|██▎       | 46/200 [00:11<00:35,  4.38it/s, epoch=45, train_loss=0.577, val_loss=1.11, epoch_time=0.222]\n",
      "Training:  23%|██▎       | 46/200 [00:11<00:35,  4.38it/s, epoch=46, train_loss=0.655, val_loss=1.22, epoch_time=0.223]\n",
      "Training:  24%|██▎       | 47/200 [00:11<00:34,  4.47it/s, epoch=46, train_loss=0.655, val_loss=1.22, epoch_time=0.223]\n",
      "Training:  24%|██▎       | 47/200 [00:11<00:34,  4.47it/s, epoch=47, train_loss=0.689, val_loss=1.07, epoch_time=0.226]\n",
      "Training:  24%|██▍       | 48/200 [00:11<00:35,  4.30it/s, epoch=47, train_loss=0.689, val_loss=1.07, epoch_time=0.226]\n",
      "Training:  24%|██▍       | 48/200 [00:11<00:35,  4.30it/s, epoch=48, train_loss=0.542, val_loss=1.14, epoch_time=0.24]\n",
      "Training:  24%|██▍       | 49/200 [00:11<00:36,  4.16it/s, epoch=48, train_loss=0.542, val_loss=1.14, epoch_time=0.24]\n",
      "Training:  24%|██▍       | 49/200 [00:11<00:36,  4.16it/s, epoch=49, train_loss=0.652, val_loss=0.995, epoch_time=0.23]\n",
      "Training:  25%|██▌       | 50/200 [00:11<00:35,  4.22it/s, epoch=49, train_loss=0.652, val_loss=0.995, epoch_time=0.23]\n",
      "Training:  25%|██▌       | 50/200 [00:12<00:35,  4.22it/s, epoch=50, train_loss=0.585, val_loss=1.07, epoch_time=0.225]\n",
      "Training:  26%|██▌       | 51/200 [00:12<00:33,  4.44it/s, epoch=50, train_loss=0.585, val_loss=1.07, epoch_time=0.225]\n",
      "Training:  26%|██▌       | 51/200 [00:12<00:33,  4.44it/s, epoch=51, train_loss=0.607, val_loss=1.01, epoch_time=0.231]\n",
      "Training:  26%|██▌       | 52/200 [00:12<00:34,  4.32it/s, epoch=51, train_loss=0.607, val_loss=1.01, epoch_time=0.231]\n",
      "Training:  26%|██▌       | 52/200 [00:12<00:34,  4.32it/s, epoch=52, train_loss=0.567, val_loss=0.924, epoch_time=0.228]\n",
      "Training:  26%|██▋       | 53/200 [00:12<00:34,  4.27it/s, epoch=52, train_loss=0.567, val_loss=0.924, epoch_time=0.228]\n",
      "Training:  26%|██▋       | 53/200 [00:12<00:34,  4.27it/s, epoch=53, train_loss=0.598, val_loss=1.04, epoch_time=0.23]\n",
      "Training:  27%|██▋       | 54/200 [00:12<00:33,  4.34it/s, epoch=53, train_loss=0.598, val_loss=1.04, epoch_time=0.23]\n",
      "Training:  27%|██▋       | 54/200 [00:13<00:33,  4.34it/s, epoch=54, train_loss=0.542, val_loss=0.842, epoch_time=0.227]\n",
      "Training:  28%|██▊       | 55/200 [00:13<00:33,  4.28it/s, epoch=54, train_loss=0.542, val_loss=0.842, epoch_time=0.227]\n",
      "Training:  28%|██▊       | 55/200 [00:13<00:33,  4.28it/s, epoch=55, train_loss=0.497, val_loss=0.895, epoch_time=0.229]\n",
      "Training:  28%|██▊       | 56/200 [00:13<00:33,  4.36it/s, epoch=55, train_loss=0.497, val_loss=0.895, epoch_time=0.229]\n",
      "Training:  28%|██▊       | 56/200 [00:13<00:33,  4.36it/s, epoch=56, train_loss=0.467, val_loss=0.988, epoch_time=0.228]\n",
      "Training:  28%|██▊       | 57/200 [00:13<00:32,  4.37it/s, epoch=56, train_loss=0.467, val_loss=0.988, epoch_time=0.228]\n",
      "Training:  28%|██▊       | 57/200 [00:13<00:32,  4.37it/s, epoch=57, train_loss=0.534, val_loss=0.8, epoch_time=0.231]\n",
      "Training:  29%|██▉       | 58/200 [00:13<00:33,  4.21it/s, epoch=57, train_loss=0.534, val_loss=0.8, epoch_time=0.231]\n",
      "Training:  29%|██▉       | 58/200 [00:14<00:33,  4.21it/s, epoch=58, train_loss=0.438, val_loss=0.717, epoch_time=0.23]\n",
      "Training:  30%|██▉       | 59/200 [00:14<00:33,  4.22it/s, epoch=58, train_loss=0.438, val_loss=0.717, epoch_time=0.23]\n",
      "Training:  30%|██▉       | 59/200 [00:14<00:33,  4.22it/s, epoch=59, train_loss=0.475, val_loss=0.722, epoch_time=0.228]\n",
      "Training:  30%|███       | 60/200 [00:14<00:31,  4.38it/s, epoch=59, train_loss=0.475, val_loss=0.722, epoch_time=0.228]\n",
      "Training:  30%|███       | 60/200 [00:14<00:31,  4.38it/s, epoch=60, train_loss=0.493, val_loss=0.683, epoch_time=0.243]\n",
      "Training:  30%|███       | 61/200 [00:14<00:34,  4.00it/s, epoch=60, train_loss=0.493, val_loss=0.683, epoch_time=0.243]\n",
      "Training:  30%|███       | 61/200 [00:14<00:34,  4.00it/s, epoch=61, train_loss=0.383, val_loss=0.686, epoch_time=0.24]\n",
      "Training:  31%|███       | 62/200 [00:14<00:33,  4.15it/s, epoch=61, train_loss=0.383, val_loss=0.686, epoch_time=0.24]\n",
      "Training:  31%|███       | 62/200 [00:15<00:33,  4.15it/s, epoch=62, train_loss=0.375, val_loss=0.725, epoch_time=0.24]\n",
      "Training:  32%|███▏      | 63/200 [00:15<00:32,  4.16it/s, epoch=62, train_loss=0.375, val_loss=0.725, epoch_time=0.24]\n",
      "Training:  32%|███▏      | 63/200 [00:15<00:32,  4.16it/s, epoch=63, train_loss=0.37, val_loss=0.804, epoch_time=0.238]\n",
      "Training:  32%|███▏      | 64/200 [00:15<00:32,  4.19it/s, epoch=63, train_loss=0.37, val_loss=0.804, epoch_time=0.238]\n",
      "Training:  32%|███▏      | 64/200 [00:15<00:32,  4.19it/s, epoch=64, train_loss=0.399, val_loss=0.774, epoch_time=0.232]\n",
      "Training:  32%|███▎      | 65/200 [00:15<00:31,  4.30it/s, epoch=64, train_loss=0.399, val_loss=0.774, epoch_time=0.232]\n",
      "Training:  32%|███▎      | 65/200 [00:15<00:31,  4.30it/s, epoch=65, train_loss=0.361, val_loss=0.764, epoch_time=0.245]\n",
      "Training:  33%|███▎      | 66/200 [00:15<00:32,  4.07it/s, epoch=65, train_loss=0.361, val_loss=0.764, epoch_time=0.245]\n",
      "Training:  33%|███▎      | 66/200 [00:15<00:32,  4.07it/s, epoch=66, train_loss=0.41, val_loss=0.707, epoch_time=0.233]\n",
      "Training:  34%|███▎      | 67/200 [00:15<00:31,  4.28it/s, epoch=66, train_loss=0.41, val_loss=0.707, epoch_time=0.233]\n",
      "Training:  34%|███▎      | 67/200 [00:16<00:31,  4.28it/s, epoch=67, train_loss=0.34, val_loss=0.745, epoch_time=0.23]\n",
      "Training:  34%|███▍      | 68/200 [00:16<00:30,  4.34it/s, epoch=67, train_loss=0.34, val_loss=0.745, epoch_time=0.23]\n",
      "Training:  34%|███▍      | 68/200 [00:16<00:30,  4.34it/s, epoch=68, train_loss=0.378, val_loss=0.704, epoch_time=0.239]\n",
      "Training:  34%|███▍      | 69/200 [00:16<00:31,  4.18it/s, epoch=68, train_loss=0.378, val_loss=0.704, epoch_time=0.239]\n",
      "Training:  34%|███▍      | 69/200 [00:16<00:31,  4.18it/s, epoch=69, train_loss=0.319, val_loss=0.682, epoch_time=0.239]\n",
      "Training:  35%|███▌      | 70/200 [00:16<00:31,  4.07it/s, epoch=69, train_loss=0.319, val_loss=0.682, epoch_time=0.239]\n",
      "Training:  35%|███▌      | 70/200 [00:16<00:31,  4.07it/s, epoch=70, train_loss=0.347, val_loss=0.766, epoch_time=0.226]\n",
      "Training:  36%|███▌      | 71/200 [00:16<00:29,  4.42it/s, epoch=70, train_loss=0.347, val_loss=0.766, epoch_time=0.226]\n",
      "Training:  36%|███▌      | 71/200 [00:17<00:29,  4.42it/s, epoch=71, train_loss=0.379, val_loss=0.661, epoch_time=0.232]\n",
      "Training:  36%|███▌      | 72/200 [00:17<00:30,  4.19it/s, epoch=71, train_loss=0.379, val_loss=0.661, epoch_time=0.232]\n",
      "Training:  36%|███▌      | 72/200 [00:17<00:30,  4.19it/s, epoch=72, train_loss=0.291, val_loss=0.784, epoch_time=0.231]\n",
      "Training:  36%|███▋      | 73/200 [00:17<00:29,  4.32it/s, epoch=72, train_loss=0.291, val_loss=0.784, epoch_time=0.231]\n",
      "Training:  36%|███▋      | 73/200 [00:17<00:29,  4.32it/s, epoch=73, train_loss=0.333, val_loss=0.752, epoch_time=0.229]\n",
      "Training:  37%|███▋      | 74/200 [00:17<00:28,  4.37it/s, epoch=73, train_loss=0.333, val_loss=0.752, epoch_time=0.229]\n",
      "Training:  37%|███▋      | 74/200 [00:17<00:28,  4.37it/s, epoch=74, train_loss=0.295, val_loss=0.656, epoch_time=0.236]\n",
      "Training:  38%|███▊      | 75/200 [00:17<00:30,  4.13it/s, epoch=74, train_loss=0.295, val_loss=0.656, epoch_time=0.236]\n",
      "Training:  38%|███▊      | 75/200 [00:18<00:30,  4.13it/s, epoch=75, train_loss=0.314, val_loss=0.734, epoch_time=0.227]\n",
      "Training:  38%|███▊      | 76/200 [00:18<00:28,  4.41it/s, epoch=75, train_loss=0.314, val_loss=0.734, epoch_time=0.227]\n",
      "Training:  38%|███▊      | 76/200 [00:18<00:28,  4.41it/s, epoch=76, train_loss=0.443, val_loss=0.776, epoch_time=0.232]\n",
      "Training:  38%|███▊      | 77/200 [00:18<00:28,  4.30it/s, epoch=76, train_loss=0.443, val_loss=0.776, epoch_time=0.232]\n",
      "Training:  38%|███▊      | 77/200 [00:18<00:28,  4.30it/s, epoch=77, train_loss=0.418, val_loss=0.699, epoch_time=0.22]\n",
      "Training:  39%|███▉      | 78/200 [00:18<00:26,  4.53it/s, epoch=77, train_loss=0.418, val_loss=0.699, epoch_time=0.22]\n",
      "Training:  39%|███▉      | 78/200 [00:18<00:26,  4.53it/s, epoch=78, train_loss=0.254, val_loss=0.652, epoch_time=0.233]\n",
      "Training:  40%|███▉      | 79/200 [00:18<00:29,  4.17it/s, epoch=78, train_loss=0.254, val_loss=0.652, epoch_time=0.233]\n",
      "Training:  40%|███▉      | 79/200 [00:19<00:29,  4.17it/s, epoch=79, train_loss=0.266, val_loss=0.689, epoch_time=0.229]\n",
      "Training:  40%|████      | 80/200 [00:19<00:27,  4.37it/s, epoch=79, train_loss=0.266, val_loss=0.689, epoch_time=0.229]\n",
      "Training:  40%|████      | 80/200 [00:19<00:27,  4.37it/s, epoch=80, train_loss=0.285, val_loss=0.703, epoch_time=0.225]\n",
      "Training:  40%|████      | 81/200 [00:19<00:26,  4.43it/s, epoch=80, train_loss=0.285, val_loss=0.703, epoch_time=0.225]\n",
      "Training:  40%|████      | 81/200 [00:19<00:26,  4.43it/s, epoch=81, train_loss=0.309, val_loss=0.627, epoch_time=0.236]\n",
      "Training:  41%|████      | 82/200 [00:19<00:28,  4.13it/s, epoch=81, train_loss=0.309, val_loss=0.627, epoch_time=0.236]\n",
      "Training:  41%|████      | 82/200 [00:19<00:28,  4.13it/s, epoch=82, train_loss=0.297, val_loss=0.819, epoch_time=0.232]\n",
      "Training:  42%|████▏     | 83/200 [00:19<00:27,  4.31it/s, epoch=82, train_loss=0.297, val_loss=0.819, epoch_time=0.232]\n",
      "Training:  42%|████▏     | 83/200 [00:19<00:27,  4.31it/s, epoch=83, train_loss=0.382, val_loss=0.858, epoch_time=0.224]\n",
      "Training:  42%|████▏     | 84/200 [00:19<00:26,  4.45it/s, epoch=83, train_loss=0.382, val_loss=0.858, epoch_time=0.224]\n",
      "Training:  42%|████▏     | 84/200 [00:20<00:26,  4.45it/s, epoch=84, train_loss=0.382, val_loss=0.651, epoch_time=0.227]\n",
      "Training:  42%|████▎     | 85/200 [00:20<00:26,  4.39it/s, epoch=84, train_loss=0.382, val_loss=0.651, epoch_time=0.227]\n",
      "Training:  42%|████▎     | 85/200 [00:20<00:26,  4.39it/s, epoch=85, train_loss=0.261, val_loss=0.68, epoch_time=0.261]\n",
      "Training:  43%|████▎     | 86/200 [00:20<00:29,  3.83it/s, epoch=85, train_loss=0.261, val_loss=0.68, epoch_time=0.261]\n",
      "Training:  43%|████▎     | 86/200 [00:20<00:29,  3.83it/s, epoch=86, train_loss=0.253, val_loss=0.697, epoch_time=0.232]\n",
      "Training:  44%|████▎     | 87/200 [00:20<00:26,  4.31it/s, epoch=86, train_loss=0.253, val_loss=0.697, epoch_time=0.232]\n",
      "Training:  44%|████▎     | 87/200 [00:20<00:26,  4.31it/s, epoch=87, train_loss=0.311, val_loss=0.754, epoch_time=0.241]\n",
      "Training:  44%|████▍     | 88/200 [00:20<00:27,  4.14it/s, epoch=87, train_loss=0.311, val_loss=0.754, epoch_time=0.241]\n",
      "Training:  44%|████▍     | 88/200 [00:21<00:27,  4.14it/s, epoch=88, train_loss=0.301, val_loss=0.736, epoch_time=0.231]\n",
      "Training:  44%|████▍     | 89/200 [00:21<00:25,  4.32it/s, epoch=88, train_loss=0.301, val_loss=0.736, epoch_time=0.231]\n",
      "Training:  44%|████▍     | 89/200 [00:21<00:25,  4.32it/s, epoch=89, train_loss=0.274, val_loss=0.643, epoch_time=0.235]\n",
      "Training:  45%|████▌     | 90/200 [00:21<00:25,  4.25it/s, epoch=89, train_loss=0.274, val_loss=0.643, epoch_time=0.235]\n",
      "Training:  45%|████▌     | 90/200 [00:21<00:25,  4.25it/s, epoch=90, train_loss=0.242, val_loss=0.632, epoch_time=0.23]\n",
      "Training:  46%|████▌     | 91/200 [00:21<00:25,  4.34it/s, epoch=90, train_loss=0.242, val_loss=0.632, epoch_time=0.23]\n",
      "Training:  46%|████▌     | 91/200 [00:21<00:25,  4.34it/s, epoch=91, train_loss=0.235, val_loss=0.62, epoch_time=0.236]\n",
      "Training:  46%|████▌     | 92/200 [00:21<00:26,  4.12it/s, epoch=91, train_loss=0.235, val_loss=0.62, epoch_time=0.236]\n",
      "Training:  46%|████▌     | 92/200 [00:22<00:26,  4.12it/s, epoch=92, train_loss=0.216, val_loss=0.593, epoch_time=0.23]\n",
      "Training:  46%|████▋     | 93/200 [00:22<00:25,  4.23it/s, epoch=92, train_loss=0.216, val_loss=0.593, epoch_time=0.23]\n",
      "Training:  46%|████▋     | 93/200 [00:22<00:25,  4.23it/s, epoch=93, train_loss=0.234, val_loss=0.673, epoch_time=0.229]\n",
      "Training:  47%|████▋     | 94/200 [00:22<00:24,  4.36it/s, epoch=93, train_loss=0.234, val_loss=0.673, epoch_time=0.229]\n",
      "Training:  47%|████▋     | 94/200 [00:22<00:24,  4.36it/s, epoch=94, train_loss=0.248, val_loss=0.704, epoch_time=0.227]\n",
      "Training:  48%|████▊     | 95/200 [00:22<00:23,  4.40it/s, epoch=94, train_loss=0.248, val_loss=0.704, epoch_time=0.227]\n",
      "Training:  48%|████▊     | 95/200 [00:22<00:23,  4.40it/s, epoch=95, train_loss=0.236, val_loss=0.775, epoch_time=0.236]\n",
      "Training:  48%|████▊     | 96/200 [00:22<00:24,  4.23it/s, epoch=95, train_loss=0.236, val_loss=0.775, epoch_time=0.236]\n",
      "Training:  48%|████▊     | 96/200 [00:23<00:24,  4.23it/s, epoch=96, train_loss=0.335, val_loss=0.788, epoch_time=0.234]\n",
      "Training:  48%|████▊     | 97/200 [00:23<00:24,  4.26it/s, epoch=96, train_loss=0.335, val_loss=0.788, epoch_time=0.234]\n",
      "Training:  48%|████▊     | 97/200 [00:23<00:24,  4.26it/s, epoch=97, train_loss=0.304, val_loss=0.648, epoch_time=0.234]\n",
      "Training:  49%|████▉     | 98/200 [00:23<00:23,  4.26it/s, epoch=97, train_loss=0.304, val_loss=0.648, epoch_time=0.234]\n",
      "Training:  49%|████▉     | 98/200 [00:23<00:23,  4.26it/s, epoch=98, train_loss=0.281, val_loss=0.623, epoch_time=0.233]\n",
      "Training:  50%|████▉     | 99/200 [00:23<00:23,  4.28it/s, epoch=98, train_loss=0.281, val_loss=0.623, epoch_time=0.233]\n",
      "Training:  50%|████▉     | 99/200 [00:23<00:23,  4.28it/s, epoch=99, train_loss=0.251, val_loss=0.691, epoch_time=0.229]\n",
      "Training:  50%|█████     | 100/200 [00:23<00:22,  4.35it/s, epoch=99, train_loss=0.251, val_loss=0.691, epoch_time=0.229]\n",
      "Training:  50%|█████     | 100/200 [00:23<00:22,  4.35it/s, epoch=100, train_loss=0.252, val_loss=0.558, epoch_time=0.235]\n",
      "Training:  50%|█████     | 101/200 [00:23<00:23,  4.15it/s, epoch=100, train_loss=0.252, val_loss=0.558, epoch_time=0.235]\n",
      "Training:  50%|█████     | 101/200 [00:24<00:23,  4.15it/s, epoch=101, train_loss=0.217, val_loss=0.583, epoch_time=0.236]\n",
      "Training:  51%|█████     | 102/200 [00:24<00:23,  4.22it/s, epoch=101, train_loss=0.217, val_loss=0.583, epoch_time=0.236]\n",
      "Training:  51%|█████     | 102/200 [00:24<00:23,  4.22it/s, epoch=102, train_loss=0.252, val_loss=0.526, epoch_time=0.234]\n",
      "Training:  52%|█████▏    | 103/200 [00:24<00:23,  4.15it/s, epoch=102, train_loss=0.252, val_loss=0.526, epoch_time=0.234]\n",
      "Training:  52%|█████▏    | 103/200 [00:24<00:23,  4.15it/s, epoch=103, train_loss=0.211, val_loss=0.622, epoch_time=0.236]\n",
      "Training:  52%|█████▏    | 104/200 [00:24<00:22,  4.23it/s, epoch=103, train_loss=0.211, val_loss=0.622, epoch_time=0.236]\n",
      "Training:  52%|█████▏    | 104/200 [00:24<00:22,  4.23it/s, epoch=104, train_loss=0.224, val_loss=0.642, epoch_time=0.231]\n",
      "Training:  52%|█████▎    | 105/200 [00:24<00:21,  4.33it/s, epoch=104, train_loss=0.224, val_loss=0.642, epoch_time=0.231]\n",
      "Training:  52%|█████▎    | 105/200 [00:25<00:21,  4.33it/s, epoch=105, train_loss=0.224, val_loss=0.62, epoch_time=0.229]\n",
      "Training:  53%|█████▎    | 106/200 [00:25<00:21,  4.36it/s, epoch=105, train_loss=0.224, val_loss=0.62, epoch_time=0.229]\n",
      "Training:  53%|█████▎    | 106/200 [00:25<00:21,  4.36it/s, epoch=106, train_loss=0.224, val_loss=0.62, epoch_time=0.233]\n",
      "Training:  54%|█████▎    | 107/200 [00:25<00:21,  4.29it/s, epoch=106, train_loss=0.224, val_loss=0.62, epoch_time=0.233]\n",
      "Training:  54%|█████▎    | 107/200 [00:25<00:21,  4.29it/s, epoch=107, train_loss=0.221, val_loss=0.598, epoch_time=0.238]\n",
      "Training:  54%|█████▍    | 108/200 [00:25<00:21,  4.20it/s, epoch=107, train_loss=0.221, val_loss=0.598, epoch_time=0.238]\n",
      "Training:  54%|█████▍    | 108/200 [00:25<00:21,  4.20it/s, epoch=108, train_loss=0.191, val_loss=0.596, epoch_time=0.251]\n",
      "Training:  55%|█████▍    | 109/200 [00:25<00:22,  3.98it/s, epoch=108, train_loss=0.191, val_loss=0.596, epoch_time=0.251]\n",
      "Training:  55%|█████▍    | 109/200 [00:26<00:22,  3.98it/s, epoch=109, train_loss=0.212, val_loss=0.608, epoch_time=0.226]\n",
      "Training:  55%|█████▌    | 110/200 [00:26<00:20,  4.41it/s, epoch=109, train_loss=0.212, val_loss=0.608, epoch_time=0.226]\n",
      "Training:  55%|█████▌    | 110/200 [00:26<00:20,  4.41it/s, epoch=110, train_loss=0.227, val_loss=0.672, epoch_time=0.23]\n",
      "Training:  56%|█████▌    | 111/200 [00:26<00:20,  4.35it/s, epoch=110, train_loss=0.227, val_loss=0.672, epoch_time=0.23]\n",
      "Training:  56%|█████▌    | 111/200 [00:26<00:20,  4.35it/s, epoch=111, train_loss=0.205, val_loss=0.67, epoch_time=0.228]\n",
      "Training:  56%|█████▌    | 112/200 [00:26<00:20,  4.39it/s, epoch=111, train_loss=0.205, val_loss=0.67, epoch_time=0.228]\n",
      "Training:  56%|█████▌    | 112/200 [00:26<00:20,  4.39it/s, epoch=112, train_loss=0.21, val_loss=0.689, epoch_time=0.237]\n",
      "Training:  56%|█████▌    | 112/200 [00:26<00:21,  4.18it/s, epoch=112, train_loss=0.21, val_loss=0.689, epoch_time=0.237]\n",
      "2026-01-11 14:41:57,407 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2026-01-11 14:41:57,407 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2026-01-11 14:41:57,407 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2026-01-11 14:42:16 Uploading - Uploading generated training model\n",
      "2026-01-11 14:42:16 Completed - Training job completed\n",
      "Training seconds: 179\n",
      "Billable seconds: 54\n",
      "Managed Spot Training savings: 69.8%\n",
      "\n",
      "training job name: pytorch-training-2026-01-11-14-38-37-129\n"
     ]
    }
   ],
   "source": [
    "estimator = PyTorch(\n",
    "    role=role,\n",
    "    framework_version=framework_version,\n",
    "    py_version=py_version,\n",
    "    source_dir=\"src\",\n",
    "    entry_point=\"preference_dynamics/sagemaker/train_cnn1d_residual.py\",\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 200,\n",
    "        \"patience\": 10,\n",
    "        \"lr\": 1e-2,\n",
    "        \"batch-size\": 32,\n",
    "        \"filters\": \"64 64 196\",\n",
    "        \"kernel-sizes\": \"3 5 7\",\n",
    "        \"hidden-dims\": \"64 96\",\n",
    "        \"dropout\": 0.3,\n",
    "    },\n",
    "    disable_profiler=True,\n",
    "    use_spot_instances=True,\n",
    "    max_run=3600,\n",
    "    max_wait=5400,\n",
    "    volume_size=50,\n",
    ")\n",
    "\n",
    "# Training data downloaded to /opt/ml/input/data/processed/\n",
    "inputs = {\"processed\": data_s3}\n",
    "estimator.fit(inputs)\n",
    "\n",
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f\"\\ntraining job name: {training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e617970",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ca832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_for_job(training_job_name: str, sm_client: BaseClient):\n",
    "    \"\"\"\n",
    "    Get model artifacts S3 path for a training job.\n",
    "\n",
    "    Args:\n",
    "        training_job_name: Name of the training job.\n",
    "        sm_client: boto3 SageMaker client.\n",
    "\n",
    "    Returns:\n",
    "        S3 path to the model artifacts.\n",
    "    \"\"\"\n",
    "    training_job = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "    return training_job[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "\n",
    "def delete_endpoints_models(sm_client: BaseClient):\n",
    "    \"\"\"\n",
    "    Delete all endpoints, endpoint configs, and models.\n",
    "\n",
    "    Args:\n",
    "        sm_client: boto3 SageMaker client.\n",
    "    \"\"\"\n",
    "    for endpoint in sm_client.list_endpoints().get(\"Endpoints\", []):\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint[\"EndpointName\"])\n",
    "        sm_client.delete_endpoint_config(EndpointConfigName=endpoint[\"EndpointName\"])\n",
    "        print(f\"Deleted endpoint: {endpoint['EndpointName']}\")\n",
    "    for model in sm_client.list_models().get(\"Models\", []):\n",
    "        sm_client.delete_model(ModelName=model[\"ModelName\"])\n",
    "        print(f\"Deleted model: {model['ModelName']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a163f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-eu-central-1-857754129070/pytorch-training-2026-01-11-14-38-37-129/output/model.tar.gz), script artifact (src/preference_dynamics/sagemaker), and dependencies ([]) into single tar.gz file located at s3://sagemaker-eu-central-1-857754129070/pytorch-inference-2026-01-11-15-10-08-767/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-inference-2026-01-11-15-10-10-402\n",
      "INFO:sagemaker:Creating endpoint-config with name pdl-cnn1d-residual-v1\n",
      "INFO:sagemaker:Creating endpoint with name pdl-cnn1d-residual-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "training_job_name = \"pytorch-training-2026-01-11-14-38-37-129\"\n",
    "model_data = model_data_for_job(training_job_name, sm)\n",
    "\n",
    "model = PyTorchModel(\n",
    "    sagemaker_session=session,\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    framework_version=framework_version,\n",
    "    py_version=py_version,\n",
    "    # Skip dependency loading for training through separate source_dir\n",
    "    source_dir=\"src/preference_dynamics/sagemaker\",\n",
    "    entry_point=\"inference_cnn1d_residual.py\",\n",
    ")\n",
    "\n",
    "\n",
    "# Deploy the model\n",
    "endpoint_name = \"pdl-cnn1d-residual-v1\"\n",
    "predictor = model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    serializer=JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6ae2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.00479126, -0.03229706,  4.22427464,  3.57283139, -9.11141968,\n",
       "         1.27821589, 11.76381111, 11.92895222]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_body = json.loads(\n",
    "    '[{\"time_series\": [[0.0, 4.130735205020703, 10.42741511136343, 13.000991632401586, 13.107347131379584, 12.428972206111084, 11.879681897898358, 11.660545016760784, 11.655366741255051, 11.71609027105237, 11.763977018052522, 11.782619408435737, 11.782715873817866, 11.77728643887746, 11.773114486025499, 11.771528825133688, 11.771546132995784], [3.8155207648866263, 2.2223991321403833, 6.6435242783096, 10.901003675676636, 12.846477694567007, 13.075174782767874, 12.675575775759418, 12.303005818271386, 12.136507385253337, 12.119354606686608, 12.155405105286086, 12.187986622354332, 12.202225796968577, 12.203477521999082, 12.20023221014977, 12.197386894106941, 12.19616613959889]], \"features\": [true, 3.6539340823159647, 2.994299404001566]}]'\n",
    ")\n",
    "\n",
    "preds = predictor.predict(\n",
    "    request_body, initial_args={\"ContentType\": \"application/json\", \"Accept\": \"application/json\"}\n",
    ")\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f774428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted endpoint: pdl-cnn1d-residual-v1\n",
      "Deleted model: pytorch-inference-2026-01-11-15-10-10-402\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "delete_endpoints_models(sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e667de",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook gives a short demonstration of model training deployment to an inference endpoint with SageMaker.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
